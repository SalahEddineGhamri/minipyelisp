* MiniPyElisp

** What ? 

Transpiler from mini python (subset of python) to elisp for education purposes


** Objectives

This project is mainly to level up my C++ game.  The language was not chosen for its convenience to the task rather for
job synergy reasons.

No offense intended for 
 + Elisp, i think it's a beautiful language, just hate those "()" 
 + nor for Python for not using all its might for this task
 + nor for C++ and my bad coding style 
 + nor for Rust, for using unsafe C++

 #+begin_quote
Maybe Emacs is already Turing complete enough. Just let Python be Python, Lisp be Lisp, and accept that your tools are secretly alive.
 #+end_quote


** The gist

Straight forward:

1. Take some Mini Python code.
2. Turn it into a Python-flavored AST (Abstract Syntax Tree). Think of it like a parse tree, but better.
3. Then, we'll generalize that into some intermediate AST format. This is where the magic happens, making it generic enough.
4. Finally, we'll convert that into an Elisp-specific AST.

If this actually works, it'll be a killer C++ workout. And the best part? We might actually manage to generate Elisp without all those ridiculous parentheses! One can dream, right?


** Build and run

to build
#+begin_src sh :results output
make
#+end_src

#+RESULTS:
: Compiling src/main.cpp...
: g++ -Wall -Wextra -std=c++17 -g -O2 -Isrc -Isrc/backend -Isrc/frontend -Isrc/intermediate -Isrc/utils -c src/main.cpp -o build/obj/src/main.o -MMD -MP
: Linking minipylisp...
: g++  build/obj/src/main.o -Wall -Wextra -std=c++17 -g -O2 -o build/minipylisp
: Build complete! Executable located at: build/minipylisp

to run the code
#+begin_src sh :results output
make run
#+end_src

#+RESULTS:
: starting minipylisp...
: Hello from minipyelisp!

to clean
#+begin_src sh :results output
make clean
#+end_src

#+RESULTS:
: removing artifacts...
: Clean done.

to build and run the tests
#+begin_src sh :results output
make test
#+end_src

#+RESULTS:
: Running tests...
: ./test_runner
: [==========] Running 2 tests from 1 test suite.
: [----------] Global test environment set-up.
: [----------] 2 tests from LexerTest
: [ RUN      ] LexerTest.TokenizeIntegerLiteral
: [       OK ] LexerTest.TokenizeIntegerLiteral (0 ms)
: [ RUN      ] LexerTest.TokenizeFloatLiteral
: [       OK ] LexerTest.TokenizeFloatLiteral (0 ms)
: [----------] 2 tests from LexerTest (0 ms total)
: 
: [----------] Global test environment tear-down
: [==========] 2 tests from 1 test suite ran. (0 ms total)
: [  PASSED  ] 2 tests.


* Step by step

** scope of the mini python
tbd

** mini python lexer
The lexer is a component responsible of turning a raw stream of character into a list of tokens. It can be found in all
of the compilers and it is the first step when dealing with any stream of characters as a source code.

We are implementing a rule based lexer that is capable of decomposing the source code into tokens based on clear
implemented rules.  Some of well implemented lexers are available already, we could have just used them but implementing
our own lexer is more fun and more educating. Some of the most used choices are Flex/Bison and ANTLR, they can be used
after defining the grammar of the language to be tokenized.

The lexer is not a parser, meaning that we do not care about the correctness of the sequence according to the language
grammar rather we care only about catching the type of the language elements, tokens. We still need a parser after this
step but with a lexer the parser is going to be a less of hustle.

Other method is to combine the lexic analysis phase and parsing into one. it is called "scannerless parsing" but it
comes with downsides and drawbacks known with little search, better stick with classical way of having a separate lexer.

** Unit testing and TDD framework

1. use unit testing to create a python lexer

#+end_src

* Project Structure and Roadmap (Assessed by Gemini, not the maintainer)

The current project structure is well-organized, following a logical separation for the frontend components.

Here's a revised view of the project structure, incorporating the next logical steps for the transpiler, and a breakdown of what needs to be done:

#+begin_src
minipyelisp/
├── build/
├── lib/
├── src/
│   ├── frontend/
│   │   ├── ast/          # Abstract Syntax Tree (AST) node definitions (Python-specific)
│   │   ├── lexer/        # Lexer implementation (already exists)
│   │   └── parser/       # Parser implementation (in progress)
│   ├── intermediate/   # Intermediate Representation (IR) layer
│   │   ├── ir_ast/     # Generic IR AST node definitions
│   │   └── ir_builder/ # Component to transform frontend AST into IR
│   ├── backend/
│   │   └── elisp_generator/ # Component to generate Elisp code from IR
│   └── main.cpp
├── tests/
│   ├── frontend/
│   │   ├── lexer_test.cpp
│   │   └── parser_test.cpp
│   ├── intermediate/
│   │   └── ir_builder_test.cpp # Tests for IR generation
│   └── backend/
│       └── elisp_generator_test.cpp # Tests for Elisp code generation
├── .gitignore
├── compile_commands.json
├── LICENSE
├── Makefile
├── README.org
└── test_runner
#+end_src

** What Needs to Be Done:

1.  **Complete the Frontend (Parser):**
    *   **Operator Precedence & Associativity:** Implement proper handling for all arithmetic, comparison, and logical operators.
    *   **More Operators:** Extend parsing to include `*`, `/`, `%`, `==`, `!=`, `<`, `<=`, `>`, `>=`, `and`, `or`, `not`.
    *   **Statements:** Implement parsing for:
        *   Assignment statements (`x = 10`)
        *   Conditional statements (`if`, `else`, `elif`)
        *   Looping constructs (`while`, `for`)
        *   Function definitions (`def`)
        *   Return statements (`return`)
        *   Function calls (`print()`, `foo(a, b)`)
    *   **Error Handling:** Add robust error reporting for syntax errors.
    *   **Indentation Handling:** Crucially, implement the logic to handle Python's significant whitespace (INDENT/DEDENT tokens from the lexer).

2.  **Develop the Intermediate Representation (IR):**
    *   **Define IR AST:** Create a set of generic AST nodes that represent the core concepts of programming languages, independent of Python or Elisp syntax. This will be the `ir_ast` component.
    *   **Build IR from Frontend AST:** Implement a component (`ir_builder`) that traverses the Python-specific AST generated by the parser and transforms it into the generic IR AST.

3.  **Implement the Backend (Elisp Generator):**
    *   **Elisp Code Generation:** Create a component (`elisp_generator`) that traverses the generic IR AST and generates valid Elisp code. This will involve mapping IR constructs to Elisp functions and forms.
    *   **Handle Elisp Conventions:** Ensure the generated Elisp adheres to common practices and avoids unnecessary parentheses where possible (as per the `README.org`'s dream!).

4.  **Comprehensive Testing:**
    *   **Parser Tests:** Expand `tests/frontend/parser_test.cpp` to cover all new parsing rules and statement types.
    *   **IR Builder Tests:** Create `tests/intermediate/ir_builder_test.cpp` to verify that the frontend AST is correctly transformed into the IR.
    *   **Elisp Generator Tests:** Create `tests/backend/elisp_generator_test.cpp` to ensure the generated Elisp code is correct and functional.
    *   **Integration Tests:** Develop end-to-end tests that take a Mini Python source file, run it through the entire transpilation process, and verify the correctness of the generated Elisp.

5.  **Define Mini Python Scope:**
    *   Clearly document the exact subset of Python features that `minipyelisp` will support. This will guide development and testing.


